pipeline {
    agent { label 'Task_Agent_1' }
    parameters {
        string(name: 'INSTANCE_ID', defaultValue: 'i-0f0d01932a34e0fd0', description: 'The EC2 instance ID to manage.')
        string(name: 'AWS_REGION', defaultValue: 'us-east-2', description: 'The AWS region where the EC2 instance is located.')
        string(name: 'S3_BUCKET', defaultValue: 'serverlogs123', description: 'S3 bucket to upload logs.')
        string(name: 'TASK_COMMAND', defaultValue: 'echo "Hello World from EC2!"', description: 'The task to execute on the EC2 instance.')
        string(name: 'LOG_FILE_PATH', defaultValue: '/var/log', description: 'Path to the log file on the EC2 instance.')
    }

    environment {
        AWS_ACCESS_KEY_ID = credentials('AWS-Cred')
        AWS_SECRET_ACCESS_KEY = credentials('AWS-Cred')
    }
    stages {
        stage('Run Task on EC2') {
            steps {
                script {
                    echo "Running task on EC2 instance ${params.INSTANCE_ID}..."
                    // Execute the task command on the EC2 instance (via SSH or Systems Manager)
                    // If you have SSH access to the instance, use SSH commands:
                    sh """
                    ssh -i /path/to/your/key.pem ec2-user@${params.INSTANCE_ID} '${params.TASK_COMMAND}'
                    """
                    // Alternatively, you can use AWS Systems Manager (SSM) to run commands if you don't want to use SSH
                    // For example, use AWS CLI to run a command using SSM:
                    sh """
                    aws ssm send-command --instance-ids ${params.INSTANCE_ID} --document-name "AWS-RunShellScript" --parameters 'commands=["${params.TASK_COMMAND}"]' --region ${params.AWS_REGION}
                    """
                }
            }
        }

        stage('Collect Logs from EC2') {
            steps {
                script {
                    echo "Collecting logs from EC2 instance ${params.INSTANCE_ID}..."

                    // Fetch logs from EC2 instance (you can use CloudWatch or directly via SSH)
                    // Assuming logs are stored in CloudWatch, you can use AWS CLI to fetch them
                    sh """
                    aws logs filter-log-events --log-group-name '/aws/ec2/instance' --log-stream-name '${params.LOG_FILE_PATH}' --region ${params.AWS_REGION} > logs.json
                    """
                    // Optionally, upload logs to S3
                    sh """
                    aws s3 cp logs.json s3://${params.S3_BUCKET}/ec2-logs/${params.INSTANCE_ID}-logs.json --region ${params.AWS_REGION}
                    """
                }
            }
        }

        stage('Stop EC2 Instance') {
            steps {
                script {
                    echo "Stopping EC2 instance ${params.INSTANCE_ID}..."
                    // Stop the EC2 instance
                    sh """
                    aws ec2 stop-instances --instance-ids ${params.INSTANCE_ID} --region ${params.AWS_REGION}
                    """
                    // Wait for the instance to stop
                    echo "Waiting for EC2 instance ${params.INSTANCE_ID} to stop..."
                    sh """
                    aws ec2 wait instance-stopped --instance-ids ${params.INSTANCE_ID} --region ${params.AWS_REGION}
                    """
                }
            }
        }
    }

    post {
        success {
            echo "Pipeline executed successfully. EC2 instance is stopped, and logs are collected."
        }
        failure {
            echo "Pipeline failed. Check the logs for more details."
        }
    }
}
